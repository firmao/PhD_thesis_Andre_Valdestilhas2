\textbf{Background}:
The concept of Linked Data relates to a collection of best practices to publish and connect structured web-based data. However, the number of available datasets has been growing significantly over the last decades \cite{bizer2011linked}. Those datasets represent now the well known as Web of Data, in which represent a large collection of concise and detailed interlinked data sets from multiple domains with large datasets \cite{saleem2013linked}. Thus, linking entries across heterogeneous data sources such as databases or knowledge bases, becomes an increasingly difficult problem \cite{valdestilhas2017high, NGAU11, saeedi2018scalable}. However, connections between datasets play a leading role in significant activities such as cross-ontology question answering \cite{lopez2009cross}, large-scale inferences \cite{urbani2010owl} and data integration \cite{rahm2016case}. In Linked Data the Linksets are well know for execute the task of generating links between datasets \cite{NGAU11}. Due to the heterogeneity of the datasets, this uniqueness is reflected in the datasets structure making a hard task to find relations among those datasets, i.e.  to identify how similar they are. In this way, we can say that Linked Data involves Datasets and Linksets and those Linksets needs to be maintained. There are many ways to maintain Linksets, one of those is to create a semantic web link repository, i.e. LinkLion \cite{linklion2014}.

\textbf{Problem}:
The current drawbacks addressed on this thesis are: Identifying and querying datasets from a huge heterogeneous collection of RDF datasets, in order to execute this task we need to assure the consistency and to know how the datasets are related and how similar they are. 

\textbf{Results}:
As results, to deal with the need for identifying LOD Datasets, we created WIMU\cite{valdestilhas2018my} a regularly updated database index of more than 660K datasets from LODStats and LOD Laundromat, an efficient, low cost and scalable service on the web that shows which dataset most likely defines a URI and various statistics of datasets indexed from LODStats and LOD Laundromat. To integrate and querying LOD datasets we provide a hybrid SPARQL query processing engine that is able to retrieve results from 559 active SPARQL endpoints (with a total of 163.23 billion triples) and 668,166 datasets (with a total of 58.49 billion triples) from LOD Stats and LOD Laudromat \cite{ValdestilhasKcap}. To assure consistency of semantic web Linked repositories where these LOD datasets are located we create an approach for the mitigation of the identifier heterogeneity problem and implement a prototype where the user is able to evaluate existing links, as well as suggest new links to be rated and a time-efficient algorithm for the detection of erroneous links in large-scale link repositories without computing all closures required by the property axiom \cite{valdestilhasdbpediasameas, valdestilhas2017cedal}. To  know how the datasets are related and how similar they are we provide a String similarity algorithm called Most Frequent K Characters, in which is based in two nested filters, (1) First Frequency Filter and (2) Hash Intersection filter, that allows discarding candidates before calculating the actual similarity value, thus giving a considerable performance gain \cite{valdestilhas2017high} and The LOD Dataset Relation Index \cite{valdestilhasSWJ2020}, in which provides information about how similar are all the datasets from LOD cloud, including statistics about the current state of those datasets.

\textbf{Conclusions}:
Our analysis showed that in order to identify and query LOD datasets there is a need to be related and to know how those datasets are related, assuring the consistency. Our analysis demonstrated that most of the datasets are disconnected from others needing to pass through a consistency and linking process in order to integrate them providing a way to query a large number of datasets simultaneously. There is a huge step towards totally queryable LOD datasets, the information contained in this thesis is an important step towards Identifying, Relating and Querying datasets on the Web of Data.
